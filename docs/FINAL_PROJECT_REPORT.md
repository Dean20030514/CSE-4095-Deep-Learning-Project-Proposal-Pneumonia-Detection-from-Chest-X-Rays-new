# Pneumonia Detection from Chest X-Rays: A Deep Learning Approach

**Course:** CSE-4095 Deep Learning  
**Date:** November 2025  
**Model:** EfficientNet-B2 (Champion Architecture)

---

## Executive Summary

This project develops a deep learning-based system for pneumonia detection from pediatric chest X-rays, achieving **96.6% accuracy** and **96.7% pneumonia recall** on the test set. We implement a rigorous experimental framework comparing five neural network architectures, optimize for medical screening requirements, and provide comprehensive error analysis with clinical context.

**Key Achievements:**
- Cleaned and restructured dataset with patient-level splits (85/10/5) to prevent data leakage
- Compared 14 experimental configurations across architectures, learning rates, and augmentation strategies
- Champion model (EfficientNet-B2 @ 384px) achieves 99.6% ROC-AUC and 99.9% PR-AUC
- Implemented threshold optimization for different clinical scenarios (screening vs. balanced vs. high-precision)
- Grad-CAM visualization confirms model focuses on medically relevant regions
- Full reproducibility with comprehensive documentation and analysis scripts

**Primary Use Case:** Educational demonstration and triage support tool (not for clinical diagnosis)

---

## 1. Introduction & Motivation

### 1.1 Global Pneumonia Burden

Pneumonia is a leading cause of death in children under 5 years old globally, accounting for **15% of all deaths** in this age group according to WHO. Early detection and treatment are critical, but:

- **Resource constraints**: Many regions lack sufficient radiologists
- **Diagnostic delays**: Manual interpretation of chest X-rays can be time-consuming
- **Inter-observer variability**: Diagnostic accuracy varies among physicians

### 1.2 Project Motivation & Scope

This project explores how deep learning can support pneumonia screening by:

1. **Automating initial triage**: Flagging high-risk cases for priority review
2. **Reducing diagnostic delays**: Providing instant preliminary assessments
3. **Supporting education**: Demonstrating AI capabilities for medical imaging students

**Important Clarifications:**
- **Target Population**: Pediatric patients (1-5 years old) from Guangzhou Women and Children's Medical Center
- **Scope**: Binary classification (pneumonia vs. normal), not multi-disease diagnosis
- **Role**: Triage support and "second reader" system, **NOT** standalone diagnostic tool
- **Limitations**: Single-center data, limited generalizability to other populations/equipment

---

## 2. Dataset & Preprocessing

### 2.1 Original Dataset Challenges

The Kaggle "Chest X-Ray Pneumonia" dataset contains **5,856 pediatric chest X-rays** with the following issues:

| Issue | Description | Impact |
|-------|-------------|--------|
| **Duplicate Images** | Multiple copies of same X-rays | Inflates performance metrics |
| **Random Splits** | No patient-level separation | Data leakage risk (same patient in train/val) |
| **Class Imbalance** | 3:1 pneumonia:normal ratio | Model bias toward majority class |
| **Unequal Splits** | 16:8:624 train:val:test ratio | Poor validation set representativeness |

### 2.2 Optimal Dataset Construction

We created a **clean, patient-level split** with the following improvements:

**Process:**
1. **De-duplication**: Removed exact duplicates using perceptual hashing
2. **Patient-level grouping**: Ensured same patient's images stay in one split
3. **Stratified sampling**: Maintained class balance across splits (85/10/5)
4. **Verification**: Confirmed no cross-contamination between splits

**Final Dataset Statistics:**

| Split | Total | Normal | Pneumonia | Split % | Pneumonia % |
|-------|-------|--------|-----------|---------|-------------|
| Train | 4,683 | 1,170 | 3,513 | 85% | 75.0% |
| Val | 589 | 148 | 441 | 10% | 74.9% |
| Test | 296 | 83 | 213 | 5% | 71.9% |

**Key Benefits:**
- No data leakage (patient-level separation)
- Representative validation set (10% for hyperparameter tuning)
- Held-out test set (5% for final evaluation)
- Stratified class distribution

*Detailed methodology: See `OPTIMAL_DATASET_REPORT.md`*

---

## 3. Methods

### 3.1 Model Architecture Comparison

We compared **5 CNN architectures** with ImageNet pre-training:

| Architecture | Parameters | Input Size | Key Features |
|--------------|-----------|------------|--------------|
| **EfficientNet-B0** | 5.3M | 224√ó224 | Efficient compound scaling |
| **EfficientNet-B2** ‚≠ê | 9.2M | 384√ó384 | Better resolution for X-rays |
| **ResNet-18** | 11.7M | 224√ó224 | Deep residual learning |
| **ResNet-50** | 25.6M | 224√ó224 | Deeper feature extraction |
| **DenseNet-121** | 8.0M | 224√ó224 | Dense connectivity |

**Champion Selection:** EfficientNet-B2 @ 384px resolution
- **Rationale**: Best balance of accuracy (96.6%), recall (96.7%), and computational efficiency
- **Advantages**: Higher resolution captures fine details crucial for X-ray interpretation
- **Trade-off**: Slightly slower inference (~2x) vs. EfficientNet-B0, but superior diagnostic performance

### 3.2 Training Strategy

**Transfer Learning:**
- Pre-trained ImageNet weights (general visual features)
- Fine-tuned all layers with lower learning rate (5e-4)
- Custom classification head for binary output

**Loss Function:**
- **Weighted Cross-Entropy**: Class weights [1.0, 1.33] to address imbalance
- **Focal Loss (optional)**: Œ≥=2.0 for hard example mining
- **Selection**: Weighted CE performed best with balanced recall

**Optimization:**
- **Optimizer**: AdamW (weight decay 1e-4)
- **Learning Rate**: 5e-4 with ReduceLROnPlateau (patience=3, factor=0.1)
- **Early Stopping**: Patience=5 epochs on macro recall
- **Batch Size**: 32 (fit in 8GB GPU memory)
- **Epochs**: Max 30 (typically converges by epoch 10-15)

**Data Augmentation:**
- Random horizontal flip (50%)
- Random rotation (¬±15¬∞)
- Color jitter (brightness ¬±20%, contrast ¬±20%)
- Random affine transformations (scale 90-110%, translate ¬±10%)

**Regularization:**
- Dropout (p=0.3) in classification head
- Weight decay (1e-4)
- Augmentation as implicit regularization

### 3.3 Evaluation Metrics & Clinical Context

**Primary Metrics (Medical Screening Priority):**

1. **Pneumonia Recall (Sensitivity)** - Most Critical
   - Measures: What % of true pneumonia cases are caught?
   - Clinical Goal: **>95%** to minimize missed diagnoses (false negatives)
   - Our Achievement: **96.7%** (206/213 cases correctly identified)

2. **Macro Recall** - Class-Balanced Performance
   - Average of pneumonia recall and normal recall
   - Ensures we don't sacrifice normal class detection
   - Our Achievement: **96.5%**

3. **ROC-AUC & PR-AUC** - Threshold-Independent Metrics
   - ROC-AUC: 99.6% (excellent discrimination)
   - PR-AUC: 99.9% (robust to class imbalance)

**Secondary Metrics:**
- **Specificity**: 96.4% (normal recall) - balance against false alarms
- **Precision**: 98.6% - when model says "pneumonia", it's right 98.6% of time
- **F1-Score**: 97.6% - harmonic mean of precision/recall
- **MCC**: 0.918 - correlation quality

**Clinical Interpretation:**

| Metric | Value | Clinical Meaning |
|--------|-------|------------------|
| True Positives (TP) | 206 | Correctly flagged pneumonia cases |
| False Negatives (FN) | 7 | **Missed pneumonia (dangerous!)** |
| True Negatives (TN) | 80 | Correctly identified normal |
| False Positives (FP) | 3 | Unnecessary follow-up (acceptable cost) |

---

## 4. Experiments & Results

### 4.1 Experimental Framework

We conducted **14 controlled experiments** across three dimensions:

**Dimension 1: Architecture Comparison (5 models)**
- EfficientNet-B0, EfficientNet-B2 ‚≠ê, ResNet-18, ResNet-50, DenseNet-121

**Dimension 2: Learning Rate Sweep (3 values)**
- 1e-4 (conservative), 5e-4 (optimal) ‚≠ê, 1e-3 (aggressive)

**Dimension 3: Augmentation Intensity (3 levels)**
- Light, Medium ‚≠ê, Aggressive

*All experiments: Same random seed, same hardware, same evaluation protocol*

### 4.2 Model Comparison Results

**Top 3 Models by Macro Recall (Validation Set):**

| Rank | Model | Macro Recall | Val Acc | Pneumonia Recall | Normal Recall | Convergence |
|------|-------|--------------|---------|------------------|---------------|-------------|
| ü•á | **EfficientNet-B2** | **98.26%** | 98.30% | 98.35% | 98.17% | Epoch 4 |
| ü•à | ResNet-18 | 97.63% | 98.47% | **99.53%** | 95.73% | Epoch 13 |
| ü•â | DenseNet-121 | 97.60% | 97.62% | 97.64% | 97.56% | Epoch 4 |

**Key Observations:**

1. **EfficientNet-B2 dominates**: Best balance of metrics and fastest convergence
2. **ResNet-18 trade-off**: Highest pneumonia recall (99.53%) but lower normal recall (95.73%)
   - Could be used in "max-sensitivity" screening mode
3. **Learning rate impact**: 5e-4 significantly outperforms 1e-4 (underfitting) and 1e-3 (instability)
4. **Augmentation sweet spot**: Medium augmentation balances regularization and information preservation

*Full experiment table: See `reports/comprehensive/EXPERIMENT_SUMMARY.md`*

### 4.3 Champion Model: Test Set Performance

**Final Evaluation (EfficientNet-B2 on held-out test set):**

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **Accuracy** | **96.62%** | Overall correctness |
| **Pneumonia Recall** | **96.71%** | 206/213 cases caught |
| **Normal Recall** | **96.39%** | 80/83 normals identified |
| **Macro Recall** | **96.55%** | Class-balanced sensitivity |
| **Macro F1** | **95.87%** | Precision-recall balance |
| **ROC-AUC** | **99.64%** | Excellent discrimination |
| **PR-AUC** | **99.86%** | Robust to imbalance |
| **MCC** | **0.918** | Strong correlation |

**Confusion Matrix:**

```
                  Predicted
                Normal  Pneumonia
Actual Normal      80       3
     Pneumonia      7     206
```

**Clinical Summary:**
- **7 false negatives** (3.3% miss rate) - primary concern for safety
- **3 false positives** (3.6% false alarm rate) - acceptable workload impact
- **206 true positives** - vast majority of pneumonia cases caught

### 4.4 Visualization: ROC & PR Curves

![ROC Curve](reports/plots/roc_curve.png)
*ROC-AUC = 99.64% indicates near-perfect separation between classes*

![PR Curve](reports/plots/pr_curve.png)
*PR-AUC = 99.86% shows robustness to 3:1 class imbalance*

### 4.5 Threshold Optimization & Operating Points

**Clinical Scenario Mapping:**

Default threshold (0.5) may not be optimal for medical screening. We swept thresholds from 0.1 to 0.9 and identified three operating points:

| Mode | Threshold | Pneumonia Recall | Precision | FN | FP | Use Case |
|------|-----------|------------------|-----------|----|----|----------|
| **Screening** | 0.10 | **99.53%** | 97.25% | 1 | 6 | Emergency room triage |
| **Balanced** | 0.50 | 96.71% | 98.56% | 7 | 3 | General screening |
| **High Precision** | 0.75 | 93.43% | 99.00% | 14 | 2 | Low-resource confirmation |

**Clinical Interpretation:**

1. **Screening Mode (0.10)**: "Catch everything, review later"
   - Only 1 missed case out of 213
   - 6 false alarms (manageable follow-up cost)
   - **Use Case**: ER triage, mass screening campaigns

2. **Balanced Mode (0.50)**: "Reasonable trade-off"
   - 7 missed cases (3.3%) - acceptable for non-critical settings
   - 3 false alarms - minimal unnecessary workload
   - **Use Case**: Routine outpatient screening

3. **High Precision Mode (0.75)**: "Minimize false alarms"
   - 14 missed cases (6.6%) - higher risk
   - Only 2 false alarms
   - **Use Case**: Resource-limited settings where follow-up is expensive

*Full threshold sweep data: `reports/best_model_test.json`*

### 4.6 Cost-Benefit Analysis: Clinical Decision Making

Beyond raw metrics, we must consider the **real-world impact** of each error type in a clinical setting.

**Cost Framework:**

| Error Type | Medical Risk | Workload Impact | Downstream Cost |
|------------|--------------|-----------------|-----------------|
| **False Negative (FN)** | **HIGH** - Missed pneumonia can lead to delayed treatment, complications, or death | Low (no follow-up triggered) | **Very High** - Potential malpractice, poor outcomes |
| **False Positive (FP)** | Low - Patient gets unnecessary follow-up imaging or review | **Moderate** - Radiologist reviews extra case (~5 min) | Low - One extra chest X-ray or clinical assessment |
| **True Positive (TP)** | None - Correct diagnosis enables timely treatment | Low - Standard workflow | None - Expected cost |
| **True Negative (TN)** | None - Correct clearance | None - No action needed | None - Expected cost |

**Quantitative Impact (Test Set, Balanced Mode @ threshold=0.50):**

| Outcome | Count | Per-Case Cost (Hypothetical) | Total Impact |
|---------|-------|------------------------------|--------------|
| **FN** | 7 | $5,000 (delayed treatment + potential complications) | $35,000 |
| **FP** | 3 | $150 (radiologist review + possible repeat imaging) | $450 |
| **TP** | 206 | $0 (standard care pathway) | $0 |
| **TN** | 80 | $0 (no action) | $0 |
| **Total** | 296 | | $35,450 |

**Cost Per Patient:** $35,450 √∑ 296 = **$119.80 per patient screened**

**Scenario Comparison:**

| Threshold | FN | FP | FN Cost | FP Cost | Total Cost | Cost/Patient |
|-----------|----|----|---------|---------|------------|--------------|
| **0.10 (Screening)** | 1 | 6 | $5,000 | $900 | $5,900 | **$19.93** |
| **0.50 (Balanced)** | 7 | 3 | $35,000 | $450 | $35,450 | **$119.80** |
| **0.75 (High Precision)** | 14 | 2 | $70,000 | $300 | $70,300 | **$237.50** |

**Key Insights:**

1. **Screening mode (0.10) is most cost-effective** despite higher FP rate
   - 6 extra reviews ($900) vastly cheaper than 6 additional missed cases ($30,000)
   - **$100 savings per patient** compared to balanced mode
   - Aligns with medical ethics: "First, do no harm" (minimize FN)

2. **High precision mode (0.75) is economically unsustainable**
   - Saves $600 on FP reviews but costs additional $35,000 in missed cases
   - Only justified in **extremely resource-constrained** settings where follow-up imaging is impossible

3. **Balanced mode (0.50) is a compromise**
   - Suitable when radiologist capacity is limited (can't handle many FP reviews)
   - Still maintains 96.7% pneumonia recall

**Operational Recommendations:**

- **Emergency Departments**: Use screening mode (0.10) - catching all critical cases is top priority
- **Outpatient Clinics**: Use balanced mode (0.50) - reasonable workload for staff
- **Mass Screening Campaigns**: Use screening mode (0.10) - cost-effective at scale
- **Low-Resource Settings (no follow-up available)**: Use balanced mode (0.50) - minimize FP to avoid impossible follow-ups

**Limitation:** Cost estimates are illustrative. Real costs vary by healthcare system, geography, and patient demographics. A formal health economics study would be needed for deployment decisions.

---

## 5. Error Analysis, Calibration & Explainability

### 5.1 Failure Mode Analysis

We manually reviewed all **10 errors** (7 FN + 3 FP) on the test set:

**False Negatives (Missed Pneumonia - 7 cases):**

| Pattern | Count | Characteristics | Example Finding |
|---------|-------|-----------------|-----------------|
| **Subtle infiltrates** | 3 | Low-contrast opacities, early-stage | Faint perihilar shadowing |
| **Image quality** | 2 | Motion blur, underexposure | Poor visualization of lung fields |
| **Atypical presentation** | 1 | Unusual location (upper lobe) | Not typical lower lobe consolidation |
| **Borderline case** | 1 | Model confidence ~0.48 | Could be normal variant |

**False Positives (Incorrect Pneumonia Flag - 3 cases):**

| Pattern | Count | Characteristics | Example Finding |
|---------|-------|-----------------|-----------------|
| **Thymus shadow** | 1 | Normal pediatric structure mimics infiltrate | Large thymus |
| **Image artifacts** | 1 | Equipment-related density variation | Grid lines, exposure gradient |
| **Vascular congestion** | 1 | Prominent vessels mistaken for infiltrate | Cardiac-related changes |

**Key Insights:**
- Model struggles with **low-contrast, early-stage pneumonia** (common challenge even for radiologists)
- **Image quality** significantly impacts performance (blur, exposure)
- Pediatric-specific anatomy (**thymus**) causes false positives
- Most errors occur near **decision boundary** (confidence 0.4-0.6)

*Detailed error gallery: `reports/error_analysis/failure_modes.json`*

### 5.2 Calibration Analysis

**What is Calibration?**
- A well-calibrated model's confidence should match actual accuracy
- Example: When model says "80% confident", it should be correct 80% of time

**Our Model's Calibration (Test Set):**

| Confidence Bin | Model Confidence | Actual Accuracy | Count | Gap |
|----------------|------------------|-----------------|-------|-----|
| 0.0 - 0.5 | 0.42 | 0.14 | 7 | +0.28 (overconfident) |
| 0.5 - 0.7 | 0.61 | 0.67 | 3 | -0.06 (underconfident) |
| 0.7 - 0.9 | 0.83 | 0.89 | 18 | -0.06 (underconfident) |
| **0.9 - 1.0** | **0.97** | **0.98** | **268** | **-0.01 (excellent)** |

**Expected Calibration Error (ECE):** 0.025 (excellent, <0.05 is well-calibrated)

**Interpretation:**
- **90%+ of predictions** have >0.9 confidence and are highly accurate
- Model is **slightly overconfident** on borderline cases (0.4-0.5 range)
- For high-stakes decisions, use **confidence thresholds** (e.g., flag <0.6 for human review)

![Calibration Plot](reports/calibration/calibration_curve.png)
*Ideal calibration: Predicted probability matches observed frequency*

### 5.3 Grad-CAM: Visual Explainability

**Method:** Gradient-weighted Class Activation Mapping highlights regions model focuses on

**Examples:**

**Case 1: True Positive (Pneumonia Correctly Identified)**

![GradCAM TP](reports/gradcam/tp_example.png)
- **Observation**: Heatmap highlights **bilateral lower lobes** and **perihilar region**
- **Clinical Validity**: Matches typical pneumonia presentation (lower lobe consolidation)
- **Confidence**: 0.98

**Case 2: True Negative (Normal Correctly Identified)**

![GradCAM TN](reports/gradcam/tn_example.png)
- **Observation**: Heatmap shows **even distribution** across lung fields
- **Clinical Validity**: Model checks entire lung but finds no focal abnormality
- **Confidence**: 0.93

**Case 3: False Negative (Missed Pneumonia)**

![GradCAM FN](reports/gradcam/fn_example.png)
- **Observation**: Heatmap focuses on **heart border** instead of **subtle right lower lobe infiltrate**
- **Issue**: Model distracted by cardiac silhouette, misses low-contrast opacity
- **Confidence**: 0.48 (borderline)

**Case 4: False Positive (Incorrect Flag)**

![GradCAM FP](reports/gradcam/fp_example.png)
- **Observation**: Heatmap highlights **thymus shadow** (normal pediatric structure)
- **Issue**: Model misinterprets normal anatomy as pathology
- **Confidence**: 0.64

**Grad-CAM Insights:**
- Model learns **clinically relevant regions** (lung parenchyma, not random backgrounds)
- Errors occur when model is **distracted by anatomical variants** (thymus, vessels)
- **Low-confidence predictions** (0.4-0.6) should trigger human review

---

## 6. Limitations & Ethical Considerations

### 6.1 Dataset Limitations

| Limitation | Impact | Mitigation |
|------------|--------|------------|
| **Single-center data** | Guangzhou hospital only | Cannot generalize to other populations |
| **Pediatric-only** | Ages 1-5 years | Does NOT work for adults |
| **Single imaging protocol** | One X-ray machine/technique | Performance drops on different equipment |
| **Binary classification** | Only pneumonia vs. normal | Misses TB, lung cancer, effusions, etc. |
| **No clinical metadata** | Missing age, sex, symptoms | Cannot do subgroup analysis |

### 6.2 Model Limitations

**Technical:**
- **7 false negatives** (3.3% miss rate) - unacceptable for standalone diagnosis
- **Calibration issues** on borderline cases (confidence 0.4-0.6)
- **Sensitivity to image quality** (blur, exposure, artifacts)
- **No uncertainty quantification** (single model, no ensemble)

**Clinical:**
- **Cannot distinguish pneumonia types** (bacterial vs. viral vs. atypical)
- **No severity grading** (mild vs. moderate vs. severe)
- **No multi-disease detection** (misses co-occurring conditions)
- **Fixed input size** (384√ó384) - may lose detail from higher-res scans

### 6.3 Ethical & Safety Considerations

**Intended Use:**
- ‚úÖ **Educational demonstration** of AI in medical imaging
- ‚úÖ **Triage support** to prioritize urgent cases in ER
- ‚úÖ **Second reader** in double-reading workflow (AI + human)

**Prohibited Use:**
- ‚ùå **Standalone diagnostic tool** without physician review
- ‚ùå **Deployment on adult patients** (trained on pediatric data only)
- ‚ùå **Use in different populations** without local validation
- ‚ùå **Legal evidence** for malpractice or reimbursement decisions

**Fairness & Bias:**
- **Population bias**: Chinese pediatric cohort only
- **Unknown subgroup performance**: No age/sex breakdown available
- **Equipment bias**: Single imaging device (may fail on portable/mobile X-rays)
- **Temporal bias**: Historical data (imaging protocols may have changed)

**Future Fairness Validation:**
- Collect demographic metadata (age, sex, ethnicity)
- Perform subgroup analysis (performance by demographics)
- Test on multi-center, multi-country datasets
- Validate on different X-ray equipment (portable, mobile, different manufacturers)

**Regulatory Compliance:**
- **Not FDA-cleared or CE-marked** - research prototype only
- Deployment requires **IRB approval** and **clinical validation study**
- Must follow **HIPAA** (US) or **GDPR** (EU) if using patient data
- Obtain **informed consent** if used in clinical research

### 6.4 Transparency & Reporting

**What We Provide:**
- ‚úÖ Full source code (training, evaluation, analysis)
- ‚úÖ Model card with limitations (`MODEL_CARD.md`)
- ‚úÖ Error analysis with failure modes
- ‚úÖ Calibration analysis and confidence guidelines
- ‚úÖ Grad-CAM visualizations for explainability

**What's Missing (Future Work):**
- ‚ùå External validation on different datasets
- ‚ùå Prospective clinical trial results
- ‚ùå Radiologist benchmark comparison
- ‚ùå Formal uncertainty quantification (Bayesian, ensemble)
- ‚ùå Subgroup fairness analysis

---

## 7. Conclusion & Future Work

### 7.1 Project Summary

This project successfully demonstrates a **rigorous, reproducible pipeline** for pneumonia detection from chest X-rays:

**Technical Achievements:**
1. **Clean dataset construction**: Patient-level splits, stratified sampling, no data leakage
2. **Comprehensive experimental framework**: 14 experiments, 5 architectures, systematic hyperparameter search
3. **Production-ready code**: Modular design, extensive documentation, automated analysis scripts
4. **Clinical validation**: Error analysis, calibration, threshold optimization, Grad-CAM explainability

**Educational Value:**
- Serves as **reference implementation** for medical imaging courses
- Demonstrates **best practices** in ML for healthcare (data splits, metrics, ethics)
- Provides **reusable components** (data pipeline, training loop, evaluation framework)

**Performance:**
- **96.6% accuracy**, **96.7% pneumonia recall** on held-out test set
- **99.6% ROC-AUC** indicates excellent discrimination capability
- **Balanced performance**: 96.4% normal recall (doesn't sacrifice specificity for sensitivity)

### 7.2 Contributions to Field

**Beyond Baseline Kaggle Implementations:**
1. **Patient-level data splitting** (prevents leakage)
2. **Threshold optimization** for clinical scenarios
3. **Comprehensive error analysis** (failure modes + Grad-CAM)
4. **Calibration analysis** (confidence reliability)
5. **Ethical framework** (limitations, fairness considerations)

**Comparison to Literature:**
- Most Kaggle solutions: 85-92% accuracy (on original flawed splits)
- Our work: 96.6% accuracy (on clean, patient-level splits)
- **Key Difference**: Rigorous methodology, not just metric chasing

### 7.3 Future Work

**Short-Term (Next Semester Project):**
1. **External validation**: Test on ChestX-ray14, MIMIC-CXR, or PadChest datasets
2. **Multi-label classification**: Detect multiple pathologies (pneumonia, effusion, cardiomegaly, etc.)
3. **Ensemble methods**: Combine multiple models for uncertainty quantification
4. **Radiologist comparison**: Benchmark against board-certified radiologists

**Medium-Term (Research Paper):**
1. **Multi-center study**: Collect data from 3-5 hospitals with diverse populations
2. **Subgroup analysis**: Performance by age, sex, disease severity
3. **Prospective validation**: Deploy in clinical setting with IRB approval
4. **Cost-effectiveness**: Model impact on diagnostic delays, outcomes, healthcare costs

**Long-Term (Clinical Deployment):**
1. **Regulatory approval**: FDA 510(k) clearance or De Novo pathway
2. **Integration**: PACS/EHR workflow, radiologist interface
3. **Monitoring**: Real-world performance tracking, model drift detection
4. **Generalization**: Adult populations, other lung diseases (TB, COVID-19)

### 7.4 Lessons Learned

**Technical:**
- **Data quality >> model complexity**: Cleaning dataset improved performance more than architecture tuning
- **Medical metrics != ML metrics**: Prioritize recall over accuracy for screening
- **Calibration matters**: Confidence scores guide when to defer to humans
- **Explainability builds trust**: Grad-CAM helps clinicians understand model decisions

**Process:**
- **Reproducibility requires effort**: Random seeds, documentation, version control
- **Automation saves time**: Scripts for experiments, analysis, reporting
- **Iterative refinement**: Error analysis guides next experiments
- **Communication is key**: Model cards, reports, visualizations for stakeholders

---

## 8. References

**Dataset:**
- Kermany, D., Zhang, K., & Goldbaum, M. (2018). Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification. *Mendeley Data*, v2.
- [Kaggle: Chest X-Ray Pneumonia](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia)

**Methods:**
- Tan, M., & Le, Q. V. (2019). EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. *ICML*.
- Selvaraju, R. R., et al. (2017). Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. *ICCV*.
- Lin, T. Y., et al. (2017). Focal Loss for Dense Object Detection. *ICCV*.

**Medical Context:**
- WHO. (2023). Pneumonia Fact Sheet. [https://www.who.int/news-room/fact-sheets/detail/pneumonia](https://www.who.int/news-room/fact-sheets/detail/pneumonia)
- Rajpurkar, P., et al. (2017). CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Neural Networks. *arXiv:1711.05225*.

**Ethics & Fairness:**
- Gianfrancesco, M. A., et al. (2018). Potential Biases in Machine Learning Algorithms Using Electronic Health Record Data. *JAMA Internal Medicine*.
- Obermeyer, Z., et al. (2019). Dissecting racial bias in an algorithm used to manage the health of populations. *Science*.

---

## Appendix: Quick Start Guide

**For Instructors/TAs:**
1. Read `PROJECT_ENHANCEMENT_SUMMARY.md` (one-page overview)
2. Check `MODEL_CARD.md` (model details, limitations, ethics)
3. Review this report for full story
4. Run demo: `streamlit run src/app/streamlit_app.py`

**For Reproducibility:**
1. Setup: `conda env create -f environment.yml`
2. Download data: `python scripts/download_sample_data.py`
3. Train: `python src/train.py --config src/configs/final_model.yaml`
4. Evaluate: `python src/eval.py --checkpoint runs/model_efficientnet_b2/best.pt`
5. Analysis: `.\scripts\complete_project_analysis.ps1`

**Repository Structure:**
```
‚îú‚îÄ‚îÄ src/                    # Core code (train, eval, models)
‚îú‚îÄ‚îÄ scripts/                # Analysis & automation
‚îú‚îÄ‚îÄ reports/                # Generated results
‚îú‚îÄ‚îÄ data/                   # Dataset (train/val/test)
‚îú‚îÄ‚îÄ docs/                   # Additional documentation
‚îî‚îÄ‚îÄ src/configs/            # Experiment configurations
```

---

**Document Version:** 1.0  
**Last Updated:** November 16, 2025  
**Contact:** [Your Name/Email]  
**Repository:** [GitHub Link]

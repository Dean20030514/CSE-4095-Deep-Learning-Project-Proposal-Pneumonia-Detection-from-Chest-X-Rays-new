# Pneumonia Xâ€‘ray Project â€” Implementation Playbook (v1.3, Academic Edition)

> Schoolâ€‘friendly revision of the v1.0 playbook with tighter alignment to the proposal, resourceâ€‘aware tracks, clearer weekâ€‘byâ€‘week plan, and deeper guidance on analysis and collaboration.

---

## Changelog (v1.1 vs v1.0)
- Added **Purpose & Motivation** paragraph linking project to global health context and triage needs.
- Introduced **Twoâ€‘Track plan**: Trackâ€‘A _MVP_ for limited resources; Trackâ€‘B _Advanced_ for stretch goals.
- Marked **advanced items** (Optuna, external validation, ONNX/TensorRT) as optional.
- Expanded **Weekâ€‘byâ€‘Week milestones** with concrete student tasks.
- Added **Collaboration** section (Git workflow, experiment logging, data handling).
- Deepened **EDA checklist** with common chestâ€‘Xâ€‘ray pitfalls & quick code patterns.
- Clarified **loss choice**: Weighted CE vs Focal (decision guide).
- Strengthened **error analysis** playbook and **calibration/thresholding** steps.
- Bundled **lowâ€‘compute configs** (Colabâ€‘friendly) and **report/poster/demo** deliverables template.

---

## 0) Purpose, Motivation & Scope (School Context)
**Problem.** Pneumonia remains a major global health burden; timely triage reduces risk of missed cases. This project builds a **3â€‘class classifier** (Normal / Bacterial / Viral) for chest Xâ€‘rays to support **educational research** and demoâ€‘level decision support.

**Primary KPI.** Maximize **pneumonia recall/sensitivity** (minimize false negatives), while tracking perâ€‘class F1, macroâ€‘F1, PRâ€‘AUC, and calibration.

**Nonâ€‘goals.** Clinical deployment or physician replacement. This is **research/education only**.

> Literature touchstones: CheXNet (dense CNNs for pneumonia), CheXpert (labeling & benchmarking), Gradâ€‘CAM (explainability), Focal Loss (imbalanced detection). See **References**.

---

## 1) Twoâ€‘Track Execution Plan
**Why two tracks?** Students often face compute/time limits. Pick Trackâ€‘A first; upgrade to Trackâ€‘B as time allows.

### Trackâ€‘A â€” MVP (Computeâ€‘Light, 1â€“2 GPUs on Colab Free/Local CPUâ€‘GPU)
- **Backbone**: EfficientNetâ€‘B0 or ResNet18 (ImageNet pretrained).
- **Image size**: 384 (start) â†’ 448 if memory allows.
- **Batch size**: 16 (AMP on GPU; 8 if OOM).
- **Loss**: Weighted Crossâ€‘Entropy; try Focal(Î³=1.5) if minority recall stalls.
- **Epochs**: 20â€“30 with EarlyStopping(patience=5â€“7) on **pneumonia recall**.
- **Augmentation**: modest (flip/rotate/brightness); keep medical realism.
- **Outputs**: baseline metrics, confusion matrix, Gradâ€‘CAM examples, Streamlit demo.

### Trackâ€‘B â€” Advanced (Stretch)
- **Backbones**: ResNet50 / EfficientNetâ€‘B3; image size 512â€“640.
- **Search**: Optuna (20â€“40 trials) over lr/weight_decay/dropout/Î³/image size.
- **External validation**: CheXpert/other CXR subset (evaluationâ€‘only).
- **Packaging**: ONNX export; Docker; optional TensorRT.

---

## 2) Resource & Platform Guidance
**Colab Free** (~T4/16GB RAM): keep 384â€“448 px, AMP on, avoid heavy augs; use gradient accumulation if needed.
**Colab Pro/Local midâ€‘range GPU**: 512 px feasible; run limited Optuna (â‰¤20 trials).
**CPUâ€‘only**: train at 224â€“320 px with ResNet18; emphasize thorough evaluation & analysis.

Tips: cache preprocessed tensors; turn off nonâ€‘essential visualizations during training; checkpoint every few epochs; reuse frozen early layers for warmâ€‘starts.

---

## 3) Collaboration & Project Hygiene
- **Git workflow**: `main` (stable) / `dev` (integration) / `feat/*` (shortâ€‘lived). Require PR + code review checklist.
- **Issues & Kanban**: tag `data`, `training`, `eval`, `ui`, `infra`. Link PRs to issues.
- **Experiment logging**: one row per run (`seed`, `config`, `metrics`, `notes`, `ckpt_path`). Use CSV/Sheets or W&B.
- **Data handling**: store **paths only** in repo; no raw data in Git. Provide `make_data.sh` and `README` for download.
- **Repro**: fix seeds; record `pip freeze`/env YAML; save **best (by pneumonia recall)** and **last** checkpoints.

---

## 4) Weekâ€‘byâ€‘Week Plan (8â€“10 Weeks)
- **W1**: Env setup; data download; quick EDA counts; define patientâ€‘safe split; decide Track.
- **W2**: Implement DataModule & transforms; finalize augmentations; class weights; baseline config file.
- **W3**: Train MVP baseline; log metrics; first confusion matrix; initial Gradâ€‘CAM sanity.
- **W4**: Error buckets (FN/FP galleries); threshold sweep (recallâ€‘first vs balanced); start calibration.
- **W5**: Ablations (image size; backbone swap); decide on Focal vs Weighted CE; finalize MVP.
- **W6**: Streamlit demo; Gradâ€‘CAM overlay; usability polish & disclaimers.
- **W7** *(optional)*: External validation and robustness tests (blur/noise/contrast curves).
- **W8â€“W9**: Report, poster, and demo video; finalize model card.
- **W10** *(buffer)*: Repro pass; packaging (ONNX/Docker if Trackâ€‘B).

---

## 5) EDA & Data Integrity Checklist
1) **Counts** per split/class; ensure triâ€‘class mapping is correct.
2) **Patient leakage**: verify no patient appears across splits.
3) **Duplicates**: dedupe by image hash (md5/sha1).
4) **Image properties**: dims/aspect ratio, grayscale vs RGB; convert grayscale to 3â€‘ch.
5) **Artifacts**: device wires/markers/text; note prevalence; flag improbable crops.
6) **Quality**: low dynamic range / overexposure; consider CLAHE cautiously.

_Quick code patterns_ (pseudocode):
```python
# class weights
w = total / (num_classes * counts)
# md5 hash
def md5(p): return hashlib.md5(open(p,'rb').read()).hexdigest()
```

---

## 6) Label Strategy (Triâ€‘Class)
- Prefer datasets/splits that distinguish **Bacterial** vs **Viral**. If Kaggle base lacks separation, curate via provided subfolders/metadata or treat as **binary pneumonia** for MVP, documenting the limitation and its impact on evaluation.

---

## 7) Handling Class Imbalance â€” Decision Guide
- Start with **Weighted Crossâ€‘Entropy** using class counts.
- If minority recall plateaus or training unstable â†’ try **Focal Loss (Î³=1â€“2)**.
- Use **WeightedRandomSampler** to balance miniâ€‘batches.
- Prefer stronger **augmentations** over synthetic generation; avoid vanilla SMOTE for images.

---

## 8) Training Recipes (Configs You Can Copy)
**Baseline (Trackâ€‘A)**
- Optimizer: AdamW (lr=3eâ€‘4, wd=1eâ€‘4)
- Scheduler: CosineAnnealingLR (T_max=epochs)
- Batch: 16 @ 384â€“448; AMP on
- Epochs: 25; EarlyStopping on **pneumonia recall**

**When to stop**: if val pneumonia recall hasnâ€™t improved â‰¥0.5% in 5 epochs and overfit rises (gap >10% between train/val), stop & adjust.

---

## 9) Evaluation, Thresholding & Calibration
- Report: accuracy; perâ€‘class P/R/F1; macroâ€‘F1; PRâ€‘AUC. Always include **pneumonia recall**.
- **Threshold sweep** on validation to produce two modes:
  - **Maxâ€‘Recall** (more positives allowed) for triage demos.
  - **Balanced** for overall F1.
- **Confusion matrix reading guide**:
  - **FP (Normalâ†’Pneumonia)**: check device shadows, text markers, poor contrast.
  - **FN (Pneumoniaâ†’Normal)**: subtle/early signs, lowâ€‘quality scans.
  - **Bacterialâ†”Viral**: overlaps; document limits and possible radiographic ambiguity.
- **Calibration**: temperature scaling if overâ€‘confident; show reliability diagram.

---

## 10) Explainability & QC
- Integrate Gradâ€‘CAM overlays; verify activations cover lung fields.
- Add simple QC rules: extreme aspect ratio; very low dynamic range; large text overlays â‡’ warn as OOD.

---

## 11) Robustness & External Validation (Optional)
- Keep a **true test** untouched during selection.
- Evaluate on a small **external subset** (e.g., CheXpert) and record domain shift.
- Stress curves: add blur/noise/contrast and plot metric degradation.

---

## 12) Deliverables (Schoolâ€‘Oriented)
1) **Short paper/report (6â€“10 pages)**: intro/motivation, related work, methods, experiments, analysis, limits/ethics, conclusion.
2) **Poster**: problem â†’ method â†’ key results â†’ CAMs â†’ takeaways.
3) **Demo video (â‰¤3 min)**: upload â†’ prediction â†’ CAM overlay â†’ maxâ€‘recall mode.
4) **Model Card** (template provided below).
5) **Repro Pack**: `config.yaml`, `state_dict.pt`, `metrics.json`, seeds, env info, and run script.

**Gradingâ€‘friendly acceptance**
- â‰¥90% accuracy (if triâ€‘class curated) or strong macroâ€‘F1; **pneumonia recall hits target**.
- Clear error analysis with galleries and at least 5 documented failure modes.
- Sensible CAMs on â‰¥90% sampled cases.

---

## 13) Ethics & Governance (Student Version)
- No PHI; strip any identifiers.
- Clear **Researchâ€‘only** disclaimer in the app.
- Document label provenance, known biases, and nonâ€‘generalizability.

---

## 14) Lowâ€‘Compute Appendix
**Colabâ€‘friendly config snippet (YAMLâ€‘ish)**
```yaml
model: efficientnet_b0
img_size: 384
batch_size: 16
epochs: 25
optimizer: adamw
lr: 3e-4
weight_decay: 1e-4
loss: weighted_ce  # switch to focal(gamma=1.5) if needed
early_stopping: {metric: pneumonia_recall, patience: 5}
aug: {flip: 0.5, rotate: 10, brightness_contrast: 0.2}
```

**Minimal training loop flags**
```bash
python -m src.train --config src/configs/effb0_384.yaml --amp --save-best-by pneumonia_recall
python -m src.eval --ckpt runs/exp_*/best.pt --split val --report reports/val.json
```

**Streamlit quickstart**
```bash
streamlit run src/app/streamlit_app.py
```

---

## 15) Model Card â€” Student Template
- **Task & Intended Use**: educational research; not a medical device.
- **Data**: sources, splits, curation rules, leakage checks.
- **Training**: backbone, size, loss, optimizer, seed.
- **Metrics**: overall + perâ€‘class; PRâ€‘AUC; calibration; thresholds.
- **Explainability**: Gradâ€‘CAM examples; sanity notes.
- **Robustness**: stress tests; external set (if any).
- **Limitations & Risks**: typical failure modes; domain shift; label noise.
- **Update Plan**: how future iterations will be assessed/documented.

---

## References (Starter List)
- Rajpurkar et al., **CheXNet** (pneumonia detection with deep CNNs)
- Irvin et al., **CheXpert** (labeling and benchmark)
- Selvaraju et al., **Gradâ€‘CAM** (visual explanations)
- Lin et al., **Focal Loss** (dense detection under imbalance)

---

### Notes
- Start simple; iterate with ablations; document everything.
- Prefer patientâ€‘level splitting whenever IDs allow.
- PRâ€‘AUC is often more informative than ROCâ€‘AUC under imbalance.



---

## Changelog (v1.2 vs v1.1)
- Added **Dayâ€‘1 Quickstart (30â€‘min)** path for zeroâ€‘friction onboarding.
- Shipped **readyâ€‘toâ€‘use config templates** (`colab_friendly.yaml`, `balanced_training.yaml`, `full_power.yaml`).
- Added **Common Issues Quick Fix** (Colab OOM, NaN loss, nonâ€‘converging, misâ€‘localized CAMs).
- Added **Showcase Guide** for a 3â€‘min video + poster layout.
- Added **Peer Review Checklist** to ensure quality and reproducibility.
- Added **Oneâ€‘Page Model Card** template for quick coursework submission.
- Provided **script stubs**: `scripts/verify_environment.py`, `scripts/download_sample_data.py`.

---

## ðŸš€ Dayâ€‘1 Quickstart (30â€‘minute Environment Check)
```bash
# 1) Clone
git clone <repo_url> && cd chestxray-pneumonia

# 2) Create environment (or open Colab notebook)
conda env create -f environment.yml && conda activate cxr
#  â€” or â€”
# Open notebooks/colab_quickstart.ipynb in Google Colab

# 3) Verify CUDA & dependencies
python scripts/verify_environment.py

# 4) Download a tiny sample (10 images) to test the pipeline
python scripts/download_sample_data.py --dest data/sample_10

# 5) Run the demo app in demo_mode
streamlit run src/app/streamlit_app.py -- --demo_mode
```

**What you should see (â‰¤30 min):** Streamlit UI loads, accepts an image from `data/sample_10/`, outputs class probabilities + Gradâ€‘CAM overlay.

---

## ðŸ“¦ Readyâ€‘toâ€‘Use Config Templates (`src/configs/`)

### `colab_friendly.yaml` â€” fastest sanity check
```yaml
model: resnet18
img_size: 224
batch_size: 16
epochs: 10
optimizer: adamw
lr: 1e-3
weight_decay: 1e-4
loss: weighted_ce
sampler: weighted_random
amp: true
early_stopping: {metric: pneumonia_recall, patience: 3}
aug: {flip: 0.5, rotate: 10}
```

### `balanced_training.yaml` â€” recommended baseline (Trackâ€‘A)
```yaml
model: efficientnet_b0
img_size: 384
batch_size: 16
epochs: 25
optimizer: adamw
lr: 3e-4
weight_decay: 1e-4
loss: weighted_ce  # switch to focal(gamma: 1.5) if minority recall stalls
scheduler: cosine
amp: true
early_stopping: {metric: pneumonia_recall, patience: 5}
aug: {flip: 0.5, rotate: 10, brightness_contrast: 0.2}
threshold_modes: {max_recall: true, balanced: true}
log: {run_name: balanced_training, csv: runs/metrics.csv}
```

### `full_power.yaml` â€” stretch (Trackâ€‘B)
```yaml
model: resnet50
img_size: 512
batch_size: 16
epochs: 40
optimizer: adamw
lr: 2e-4
weight_decay: 2e-4
loss: focal
focal: {gamma: 1.5}
scheduler: cosine
amp: true
optuna: {trials: 30, search_space: [lr, weight_decay, dropout, img_size]}
early_stopping: {metric: pneumonia_recall, patience: 7}
external_eval: {dataset: chexpert_small, enabled: true}
export: {onnx: true}
```

---

## ðŸ§° Common Issues â€” Quick Fix
**Colab OOM** â†’ `batch_size: 8` â†’ `img_size: 224` â†’ `gradient_accumulation: 2` â†’ disable heavy augs.

**Loss becomes NaN** â†’ lower LR (e.g., `3e-4 â†’ 1e-4`) â†’ check normalization pipeline â†’ temporarily disable risky augs.

**Model wonâ€™t converge** â†’ freeze early backbone layers (`requires_grad=False`) for 3â€“5 epochs â†’ simplify to binary (`pneumonia vs normal`) to debug â†’ verify label mapping and class weights.

**CAM heatmaps offâ€‘lung** â†’ ensure **identical** preprocessing in train vs eval â†’ verify model loads the intended weights â†’ sample CAMs from **unseen** images.

---

## ðŸŽ¬ Showcase Guide â€” 3â€‘minute Video & Poster
**Video outline (â‰¤3:00)**
- 0:00â€“0:30 Problem & societal impact
- 0:30â€“1:15 Typical cases + CAM overlays
- 1:15â€“2:00 Thresholding: *Maxâ€‘Recall* vs *Balanced* modes
- 2:00â€“2:30 1â€“2 tough failures & lessons
- 2:30â€“3:00 Limits & future work

**Poster hotspots**
- **Topâ€‘left:** clinical motivation + data stats chart
- **Center:** model diagram + CAM comparisons
- **Bottomâ€‘right:** metrics table (highlight recall) + key takeaways

---

## ðŸ‘¥ Peer Review Checklist (Quality & Repro)
- [ ] Dataloader handles corrupted images without crashing (skips & logs)
- [ ] Train/resume/save/load cycle verified; bestâ€‘by **pneumonia recall** saved
- [ ] All randomness controlled by `seed` (torch, numpy, python, dataloader workers)
- [ ] Eval and train preprocessing **identical**
- [ ] Gradâ€‘CAM works on heldâ€‘out images; lungâ€‘field sanity pass
- [ ] Streamlit demo includes **researchâ€‘only** disclaimer
- [ ] `metrics.csv` or W&B run contains config + metrics + ckpt path

---

## ðŸ“‹ Oneâ€‘Page Model Card (Course Submission)
**Task & Use**: triâ€‘class pneumonia classifier for **educational research**; not for clinical use.

**Data**: sources, curation, patientâ€‘level split, dedup & leakage checks.

**Training**: backbone, img_size, loss, optimizer, epochs, seed; config file name.

**Metrics**: overall & perâ€‘class P/R/F1, macroâ€‘F1, PRâ€‘AUC; **thresholded modes** (*maxâ€‘recall* & *balanced*); calibration.

**Explainability**: 2â€“3 Gradâ€‘CAM panels with notes.

**Limits & Risks**: top 3â€“5 failure modes; domain shift; label noise.

**Contacts/Repo**: team lead name/email; repository URL.

---

## ðŸ§ª Script Stubs

### `scripts/verify_environment.py`
```python
import torch, pkgutil, sys
print("CUDA available:", torch.cuda.is_available())
print("CUDA device:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU")
required = ["torch", "torchvision", "numpy", "pandas", "albumentations", "sklearn", "streamlit"]
missing = [m for m in required if pkgutil.find_loader(m) is None]
print("Missing packages:", missing)
if missing:
    sys.exit(1)
print("OK")
```

### `scripts/download_sample_data.py`
```python
import argparse, os, shutil, urllib.request, zipfile
URL = "https://storage.googleapis.com/quickstart-cxr/sample_10.zip"  # replace with real link
parser = argparse.ArgumentParser()
parser.add_argument('--dest', default='data/sample_10')
args = parser.parse_args()
os.makedirs(args.dest, exist_ok=True)
zip_path = os.path.join(args.dest, 'sample_10.zip')
urllib.request.urlretrieve(URL, zip_path)
with zipfile.ZipFile(zip_path) as zf:
    zf.extractall(args.dest)
os.remove(zip_path)
print("Sample downloaded to", args.dest)
```

> **Tip**: Put real dataset links in `README_data.md` and keep raw data **out of Git**; store only paths/scripts.



---

## Changelog (v1.3 vs v1.2)
- Added **Project Difficulty Selfâ€‘Assessment** (entry/advanced/research tracks) to help students pick scope.
- Shipped **Demo Presentation Script Template** (`scripts/demo_presentation.py`) to autoâ€‘generate key figures.
- Added **Peer Assessment Questionnaire** for intraâ€‘team review.
- Added **Report Section Template** with page/word guidance for 8â€‘page paper.
- Added **Demo Day Final Checklist** (tech + speaking readiness).
- Added **Grading Rubric Mapping** table aligning course criteria to evidence in the repo.

---

## ðŸ“Š Project Difficulty Selfâ€‘Assessment
Choose your lane based on time/compute/experience:
- **Entry (Undergrad course project)**: Trackâ€‘A MVP **+** binary classification **+** basic CAM **+** short report.
- **Intermediate (Capstone/Master)**: Full Trackâ€‘A **+** triâ€‘class **+** deeper error analysis **+** model card.
- **Advanced (Publicationâ€‘oriented)**: Trackâ€‘B **+** external validation **+** ablations **+** paper draft.

> Tip: Start Entry, lock your baseline by Weekâ€‘3, then promote scope only if velocity and compute allow.

---

## ðŸ–¥ï¸ Demo Presentation Script Template (`scripts/demo_presentation.py`)
```python
"""Autoâ€‘generate presentation assets: metrics plots, confusion matrix, CAM grid, threshold curves."""
from pathlib import Path
import json, itertools
import numpy as np
import matplotlib.pyplot as plt

FIG_DIR = Path("reports/figs"); FIG_DIR.mkdir(parents=True, exist_ok=True)

# 1) Bar chart: metrics comparison across runs

def plot_metric_bars(metrics_jsons, metric="macro_f1", labels=None, outfile="metric_bars.png"):
    runs = [json.load(open(p)) for p in metrics_jsons]
    vals = [r.get(metric, np.nan) for r in runs]
    labels = labels or [Path(p).stem for p in metrics_jsons]
    plt.figure()
    plt.bar(range(len(vals)), vals)
    plt.xticks(range(len(vals)), labels, rotation=15)
    plt.ylabel(metric)
    plt.title(f"Comparison â€” {metric}")
    plt.tight_layout(); plt.savefig(FIG_DIR/outfile, dpi=200); plt.close()

# 2) Confusion matrix (expects 3x3 or 2x2)

def plot_confusion_matrix(cm, classes, normalize=False, outfile="confusion_matrix.png"):
    cm = np.array(cm).astype(float)
    if normalize:
        cm = cm / cm.sum(axis=1, keepdims=True).clip(min=1e-9)
    plt.figure()
    plt.imshow(cm, interpolation='nearest')
    plt.title("Confusion Matrix (val)"); plt.colorbar()
    tick = np.arange(len(classes)); plt.xticks(tick, classes); plt.yticks(tick, classes)
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, f"{cm[i, j]:.2f}", ha="center", va="center")
    plt.tight_layout(); plt.savefig(FIG_DIR/outfile, dpi=200); plt.close()

# 3) Threshold sweep (precisionâ€‘recall vs threshold)

def plot_threshold_curve(thresholds, recalls, precisions, outfile="threshold_curve.png"):
    plt.figure()
    plt.plot(thresholds, recalls, label="Recall")
    plt.plot(thresholds, precisions, label="Precision")
    plt.xlabel("Threshold"); plt.legend(); plt.title("Threshold Sweep")
    plt.tight_layout(); plt.savefig(FIG_DIR/outfile, dpi=200); plt.close()

# 4) Placeholder for CAM grid (fill with your projectâ€™s CAM function)

def save_cam_grid(images, cams, cols=4, outfile="cam_grid.png"):
    rows = int(np.ceil(len(images)/cols))
    plt.figure(figsize=(cols*3, rows*3))
    for i, (img, cam) in enumerate(zip(images, cams)):
        ax = plt.subplot(rows, cols, i+1)
        ax.imshow(img, cmap="gray")
        ax.imshow(cam, alpha=0.35)
        ax.axis('off')
    plt.tight_layout(); plt.savefig(FIG_DIR/outfile, dpi=200); plt.close()

if __name__ == "__main__":
    # Example usage (replace with real paths/arrays)
    # plot_metric_bars(["reports/val_run1.json", "reports/val_run2.json"], metric="macro_f1")
    pass
```

---

## ðŸ‘¥ Peer Assessment Questionnaire (Internal)
**Code Quality (1â€“5)**
- Clear docstrings & comments
- Singleâ€‘responsibility functions; modular structure
- Robust error handling; no silent crashes

**Experimental Rigor (1â€“5)**
- Full reproducibility (fixed seeds)
- Proper validation without leakage
- Justified hyperparameter choices

**Analytical Depth (1â€“5)**
- Specific & insightful error analysis
- Honest limitations & ethics
- Visualizations that support claims

> Scoring: average each section; flag any item â‰¤2 for action before submission.

---

## ðŸ“ Report Section Template (8 pages)
1. **Introduction (1 page)** â€” background, significance, objectives
2. **Related Work (1 page)** â€” key papers & route rationale
3. **Methods (2 pages)** â€” data, architecture, training, imbalance handling
4. **Experiments (2 pages)** â€” setup, results, analysis, visualizations
5. **Discussion (1.5 pages)** â€” limits, ethics, future work
6. **Conclusion (0.5 page)** â€” contributions & takeaways

> Guideline: ~800â€“1000 words per full page (with figures reducing word count).

---

## ðŸŽ¤ Demo Day â€” Final Checklist
**Tech**
- Backup weights & code; verify **offline demo** path
- Projector/browser tested; fallback plan ready
- 3 typical cases + 1 edge case queued

**Speaking**
- Timeboxed rehearsal (â‰¤3:00); 1â€‘minute elevator pitch
- Prepare 3 likely technical Qs with succinct answers
- Oneâ€‘pager quick reference for the panel

---

## ðŸ“‹ Grading Rubric Mapping
| Dimension | Playbook Sections | Evidence |
|---|---|---|
| Technical depth | Â§1, Â§7, Â§8 | Model choice rationale; ablation records |
| Experimental rigor | Â§3, Â§5, Â§9 | Data splits; evaluation protocol; reproducibility checklist |
| Analysis quality | Â§9, Â§10 | Error galleries; CAM explanations; calibration curves |
| Engineering practice | Â§3, Â§14 | Code structure; scripts/configs; CI/checklists |
| Academic integrity | Â§13, Â§15 | Ethics statement; citations; model card |

> Place links to concrete artifacts (e.g., `reports/val.json`, `reports/figs/*`, `runs/metrics.csv`) in your README.


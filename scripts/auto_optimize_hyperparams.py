"""
è‡ªåŠ¨è¶…å‚æ•°ä¼˜åŒ–è„šæœ¬

åŠŸèƒ½ï¼š
1. æ ¹æ®è®­ç»ƒç»“æœè‡ªåŠ¨è°ƒæ•´è¶…å‚æ•°
2. è¿­ä»£è®­ç»ƒç›´åˆ°æ‰¾åˆ°æœ€ä¼˜é…ç½®
3. ä¿å­˜ä¼˜åŒ–å†å²å’Œæœ€ä½³æ¨¡å‹

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/auto_optimize_hyperparams.py --config src/configs/baseline.yaml --iterations 10 --target pneumonia_recall

ç­–ç•¥ï¼š
- å¦‚æœæ€§èƒ½æå‡ â†’ æ²¿å½“å‰æ–¹å‘ç»§ç»­è°ƒæ•´
- å¦‚æœæ€§èƒ½ä¸‹é™ â†’ åå‘è°ƒæ•´æˆ–å°è¯•å…¶ä»–å‚æ•°
- è‡ªåŠ¨è®°å½•æ‰€æœ‰å°è¯•ï¼Œé¿å…é‡å¤

ä½œè€…ï¼šCSE-4095 Deep Learning Team
"""

import argparse
import json
import yaml
import subprocess
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple
import numpy as np
from datetime import datetime
import shutil


class HyperparameterOptimizer:
    """è‡ªåŠ¨è¶…å‚æ•°ä¼˜åŒ–å™¨"""
    
    def __init__(self, base_config_path: str, target_metric: str = 'macro_recall',
                 output_dir: str = 'runs/auto_optimization'):
        """
        Args:
            base_config_path: åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„
            target_metric: ä¼˜åŒ–ç›®æ ‡æŒ‡æ ‡ (macro_recall, pneumonia_recall, val_accç­‰)
            output_dir: è¾“å‡ºç›®å½•
        """
        self.base_config_path = Path(base_config_path)
        self.target_metric = target_metric
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½åŸºç¡€é…ç½®
        with open(self.base_config_path, 'r', encoding='utf-8') as f:
            self.base_config = yaml.safe_load(f)
        
        # ä¼˜åŒ–å†å²
        self.history = []
        self.best_config = None
        self.best_score = -1.0
        self.iteration = 0
        
        # å‚æ•°æœç´¢ç©ºé—´ï¼ˆå®šä¹‰æ¯ä¸ªå‚æ•°çš„å¯é€‰å€¼å’Œè°ƒæ•´ç­–ç•¥ï¼‰
        self.param_space = {
            'lr': {
                'values': [0.0001, 0.0003, 0.0005, 0.001, 0.003],
                'type': 'continuous',
                'scale': 'log'
            },
            'batch_size': {
                'values': [8, 16, 32],
                'type': 'discrete'
            },
            'augment_level': {
                'values': ['light', 'medium', 'aggressive'],
                'type': 'categorical'
            },
            'model': {
                'values': ['resnet18', 'densenet121', 'efficientnet_b0'],
                'type': 'categorical'
            },
            'img_size': {
                'values': [224, 384],
                'type': 'discrete'
            }
        }
        
        # å½“å‰æ¢ç´¢çš„å‚æ•°
        self.current_param_to_optimize = 'lr'
        
        print(f"[AutoOptimizer] åˆå§‹åŒ–å®Œæˆ")
        print(f"  - åŸºç¡€é…ç½®: {base_config_path}")
        print(f"  - ç›®æ ‡æŒ‡æ ‡: {target_metric}")
        print(f"  - è¾“å‡ºç›®å½•: {output_dir}")
    
    def get_next_config(self) -> Dict:
        """
        æ ¹æ®å†å²ç»“æœç”Ÿæˆä¸‹ä¸€ä¸ªé…ç½®
        
        ç­–ç•¥ï¼š
        1. å‰3æ¬¡ï¼šå¿«é€Ÿæ¢ç´¢ä¸åŒå­¦ä¹ ç‡
        2. æ‰¾åˆ°è¾ƒå¥½çš„lråï¼šæ¢ç´¢augmentation
        3. å†æ¢ç´¢modelæ¶æ„
        4. æœ€åå¾®è°ƒbatch_sizeå’Œimg_size
        """
        new_config = self.base_config.copy()
        
        if self.iteration == 0:
            # ç¬¬ä¸€æ¬¡ï¼šä½¿ç”¨åŸºç¡€é…ç½®
            return new_config
        
        # æ ¹æ®è¿­ä»£æ¬¡æ•°é€‰æ‹©ä¼˜åŒ–ç­–ç•¥
        if self.iteration <= 3:
            # å‰3æ¬¡ï¼šæ¢ç´¢å­¦ä¹ ç‡
            lr_values = self.param_space['lr']['values']
            new_config['lr'] = lr_values[min(self.iteration - 1, len(lr_values) - 1)]
            print(f"  [æ¢ç´¢å­¦ä¹ ç‡] lr={new_config['lr']}")
        
        elif self.iteration <= 6:
            # ç¬¬4-6æ¬¡ï¼šä½¿ç”¨æœ€ä½³lrï¼Œæ¢ç´¢augmentation
            if self.best_config:
                new_config['lr'] = self.best_config.get('lr', self.base_config['lr'])
            
            aug_values = self.param_space['augment_level']['values']
            aug_idx = (self.iteration - 4) % len(aug_values)
            new_config['augment_level'] = aug_values[aug_idx]
            print(f"  [æ¢ç´¢å¢å¼º] augment_level={new_config['augment_level']}")
        
        elif self.iteration <= 9:
            # ç¬¬7-9æ¬¡ï¼šä½¿ç”¨æœ€ä½³lrå’Œaugï¼Œæ¢ç´¢æ¨¡å‹æ¶æ„
            if self.best_config:
                new_config['lr'] = self.best_config.get('lr', self.base_config['lr'])
                new_config['augment_level'] = self.best_config.get('augment_level', 'medium')
            
            model_values = self.param_space['model']['values']
            model_idx = (self.iteration - 7) % len(model_values)
            new_config['model'] = model_values[model_idx]
            print(f"  [æ¢ç´¢æ¶æ„] model={new_config['model']}")
        
        else:
            # ä¹‹åï¼šåŸºäºæœ€ä½³é…ç½®è¿›è¡Œå¾®è°ƒ
            if self.best_config:
                new_config = self.best_config.copy()
                # éšæœºå¾®è°ƒæŸä¸ªå‚æ•°
                param_to_tune = np.random.choice(['batch_size', 'img_size'])
                values = self.param_space[param_to_tune]['values']
                current_val = new_config.get(param_to_tune, values[0])
                
                # å°è¯•ä¸åŒçš„å€¼
                if current_val in values:
                    idx = values.index(current_val)
                    new_idx = (idx + 1) % len(values)
                    new_config[param_to_tune] = values[new_idx]
                
                print(f"  [å¾®è°ƒ] {param_to_tune}={new_config[param_to_tune]}")
        
        return new_config
    
    def run_training(self, config: Dict) -> Dict:
        """
        è¿è¡Œä¸€æ¬¡è®­ç»ƒ
        
        Returns:
            åŒ…å«è®­ç»ƒç»“æœçš„å­—å…¸
        """
        # ä¿å­˜é…ç½®åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_config_path = self.output_dir / f'config_iter{self.iteration}.yaml'
        with open(temp_config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f)
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        run_dir = self.output_dir / f'iteration_{self.iteration}'
        config['output_dir'] = str(run_dir)
        
        # æ›´æ–°é…ç½®æ–‡ä»¶
        with open(temp_config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f)
        
        print(f"\n{'='*60}")
        print(f"è¿­ä»£ {self.iteration}: å¼€å§‹è®­ç»ƒ")
        print(f"{'='*60}")
        print(f"é…ç½®: {temp_config_path}")
        print(f"è¾“å‡º: {run_dir}")
        
        # æ„å»ºè®­ç»ƒå‘½ä»¤
        cmd = [
            'python', 'src/train.py',
            '--config', str(temp_config_path)
        ]
        
        # è¿è¡Œè®­ç»ƒ
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='ignore'
            )
            
            if result.returncode != 0:
                print(f"[ERROR] è®­ç»ƒå¤±è´¥: {result.stderr}")
                return None
            
            print(f"[SUCCESS] è®­ç»ƒå®Œæˆ")
            
        except Exception as e:
            print(f"[ERROR] è¿è¡Œå¤±è´¥: {e}")
            return None
        
        # è¯»å–è®­ç»ƒç»“æœ
        return self.extract_results(run_dir)
    
    def extract_results(self, run_dir: Path) -> Dict:
        """
        ä»è®­ç»ƒè¾“å‡ºä¸­æå–ç»“æœ
        
        Args:
            run_dir: è®­ç»ƒè¾“å‡ºç›®å½•
        
        Returns:
            åŒ…å«æŒ‡æ ‡çš„å­—å…¸
        """
        metrics_file = run_dir / 'metrics_history.csv'
        
        if not metrics_file.exists():
            print(f"[WARNING] æ‰¾ä¸åˆ°metricsæ–‡ä»¶: {metrics_file}")
            return None
        
        try:
            df = pd.read_csv(metrics_file)
            
            # è·å–æœ€ä½³epochçš„æŒ‡æ ‡
            if self.target_metric in df.columns:
                best_idx = df[self.target_metric].idxmax()
                best_row = df.loc[best_idx]
                
                results = {
                    'epoch': int(best_row['epoch']),
                    'val_acc': float(best_row.get('val_acc', 0)),
                    'macro_recall': float(best_row.get('macro_recall', 0)),
                    'pneumonia_recall': float(best_row.get('pneumonia_recall', 0)),
                    'val_loss': float(best_row.get('val_loss', 0)),
                    self.target_metric: float(best_row[self.target_metric])
                }
                
                return results
            else:
                print(f"[WARNING] æ‰¾ä¸åˆ°ç›®æ ‡æŒ‡æ ‡: {self.target_metric}")
                return None
                
        except Exception as e:
            print(f"[ERROR] è¯»å–ç»“æœå¤±è´¥: {e}")
            return None
    
    def update_history(self, config: Dict, results: Dict):
        """æ›´æ–°ä¼˜åŒ–å†å²"""
        if results is None:
            return
        
        score = results.get(self.target_metric, 0)
        
        # è®°å½•å†å²
        entry = {
            'iteration': self.iteration,
            'config': config.copy(),
            'results': results.copy(),
            'score': score,
            'timestamp': datetime.now().isoformat()
        }
        self.history.append(entry)
        
        # æ›´æ–°æœ€ä½³é…ç½®
        if score > self.best_score:
            self.best_score = score
            self.best_config = config.copy()
            
            print(f"\nğŸ‰ æ–°çš„æœ€ä½³é…ç½®!")
            print(f"  {self.target_metric}: {score:.4f} (â†‘ {score - self.best_score:.4f})")
            
            # ä¿å­˜æœ€ä½³é…ç½®
            best_config_path = self.output_dir / 'best_config.yaml'
            with open(best_config_path, 'w', encoding='utf-8') as f:
                yaml.dump(self.best_config, f)
            
            # å¤åˆ¶æœ€ä½³æ¨¡å‹
            src_model = Path(config['output_dir']) / 'best_model.pt'
            dst_model = self.output_dir / 'best_model.pt'
            if src_model.exists():
                shutil.copy(src_model, dst_model)
                print(f"  æ¨¡å‹å·²ä¿å­˜: {dst_model}")
    
    def save_history(self):
        """ä¿å­˜ä¼˜åŒ–å†å²"""
        history_file = self.output_dir / 'optimization_history.json'
        
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(self.history, f, indent=2, ensure_ascii=False)
        
        print(f"\n[ä¿å­˜] ä¼˜åŒ–å†å²: {history_file}")
        
        # ç”ŸæˆCSVæ‘˜è¦
        summary_data = []
        for entry in self.history:
            row = {
                'iteration': entry['iteration'],
                'score': entry['score'],
            }
            # æ·»åŠ ä¸»è¦é…ç½®å‚æ•°
            for param in ['lr', 'model', 'augment_level', 'batch_size', 'img_size']:
                row[param] = entry['config'].get(param, 'N/A')
            
            # æ·»åŠ ä¸»è¦ç»“æœ
            if entry['results']:
                for metric in ['val_acc', 'macro_recall', 'pneumonia_recall']:
                    row[metric] = entry['results'].get(metric, 0)
            
            summary_data.append(row)
        
        summary_df = pd.DataFrame(summary_data)
        summary_file = self.output_dir / 'optimization_summary.csv'
        summary_df.to_csv(summary_file, index=False)
        
        print(f"[ä¿å­˜] ä¼˜åŒ–æ‘˜è¦: {summary_file}")
        
        # æ‰“å°Top 3é…ç½®
        if len(summary_data) > 0:
            print(f"\n{'='*60}")
            print(f"Top 3 é…ç½®:")
            print(f"{'='*60}")
            top3 = summary_df.nlargest(3, 'score')
            print(top3.to_string(index=False))
    
    def optimize(self, max_iterations: int = 10):
        """
        è¿è¡Œä¼˜åŒ–å¾ªç¯
        
        Args:
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        """
        print(f"\n{'='*60}")
        print(f"å¼€å§‹è‡ªåŠ¨è¶…å‚æ•°ä¼˜åŒ–")
        print(f"{'='*60}")
        print(f"æœ€å¤§è¿­ä»£æ¬¡æ•°: {max_iterations}")
        print(f"ç›®æ ‡æŒ‡æ ‡: {self.target_metric}")
        print(f"\n")
        
        for i in range(max_iterations):
            self.iteration = i
            
            # ç”Ÿæˆä¸‹ä¸€ä¸ªé…ç½®
            config = self.get_next_config()
            
            # è¿è¡Œè®­ç»ƒ
            results = self.run_training(config)
            
            # æ›´æ–°å†å²
            self.update_history(config, results)
            
            # ä¿å­˜å½“å‰è¿›åº¦
            self.save_history()
            
            print(f"\nå½“å‰è¿›åº¦: {i+1}/{max_iterations}")
            if self.best_score > 0:
                print(f"æœ€ä½³ {self.target_metric}: {self.best_score:.4f}")
        
        # æœ€ç»ˆæŠ¥å‘Š
        self.print_final_report()
    
    def print_final_report(self):
        """æ‰“å°æœ€ç»ˆä¼˜åŒ–æŠ¥å‘Š"""
        print(f"\n{'='*60}")
        print(f"ä¼˜åŒ–å®Œæˆ!")
        print(f"{'='*60}")
        
        if self.best_config:
            print(f"\nğŸ† æœ€ä½³é…ç½®:")
            print(f"  {self.target_metric}: {self.best_score:.4f}")
            print(f"\né…ç½®è¯¦æƒ…:")
            for key, value in self.best_config.items():
                if key not in ['output_dir']:
                    print(f"  {key}: {value}")
            
            print(f"\næœ€ä½³é…ç½®å·²ä¿å­˜:")
            print(f"  é…ç½®: {self.output_dir / 'best_config.yaml'}")
            print(f"  æ¨¡å‹: {self.output_dir / 'best_model.pt'}")
            print(f"  å†å²: {self.output_dir / 'optimization_history.json'}")
            print(f"  æ‘˜è¦: {self.output_dir / 'optimization_summary.csv'}")
            
            # ç»™å‡ºä½¿ç”¨å»ºè®®
            print(f"\nğŸ“ ä½¿ç”¨æœ€ä½³é…ç½®:")
            print(f"  python src/train.py --config {self.output_dir / 'best_config.yaml'}")
        else:
            print("\nâš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆé…ç½®")


def main():
    parser = argparse.ArgumentParser(description='è‡ªåŠ¨è¶…å‚æ•°ä¼˜åŒ–')
    parser.add_argument('--config', required=True, help='åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--iterations', type=int, default=10, help='æœ€å¤§è¿­ä»£æ¬¡æ•°')
    parser.add_argument('--target', default='macro_recall', 
                       choices=['macro_recall', 'pneumonia_recall', 'val_acc', 'macro_f1'],
                       help='ä¼˜åŒ–ç›®æ ‡æŒ‡æ ‡')
    parser.add_argument('--output_dir', default='runs/auto_optimization', 
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = HyperparameterOptimizer(
        base_config_path=args.config,
        target_metric=args.target,
        output_dir=args.output_dir
    )
    
    # è¿è¡Œä¼˜åŒ–
    optimizer.optimize(max_iterations=args.iterations)


if __name__ == '__main__':
    main()


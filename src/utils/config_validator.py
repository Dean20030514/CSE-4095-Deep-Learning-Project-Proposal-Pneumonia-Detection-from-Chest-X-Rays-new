"""
é…ç½®æ–‡ä»¶éªŒè¯æ¨¡å—

æä¾›é…ç½®æ–‡ä»¶çš„å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§æ£€æŸ¥ï¼Œé˜²æ­¢è¿è¡Œæ—¶é”™è¯¯ã€‚
æ”¯æŒ GPU æ˜¾å­˜ä¼°ç®—å’Œé…ç½®ç»§æ‰¿ã€‚
"""
from typing import Dict, Any, List, Optional, Tuple
import yaml
from pathlib import Path


# æ¨¡å‹çš„å‚æ•°é‡å’Œæ¨è batch sizeï¼ˆåŸºäº 8GB GPUï¼‰
MODEL_SPECS = {
    'resnet18': {'params_m': 11.7, 'base_memory_mb': 500, 'recommended_batch_8gb': 64},
    'resnet50': {'params_m': 25.6, 'base_memory_mb': 1000, 'recommended_batch_8gb': 32},
    'efficientnet_b0': {'params_m': 5.3, 'base_memory_mb': 400, 'recommended_batch_8gb': 64},
    'efficientnet_b2': {'params_m': 9.2, 'base_memory_mb': 600, 'recommended_batch_8gb': 48},
    'densenet121': {'params_m': 8.0, 'base_memory_mb': 700, 'recommended_batch_8gb': 48},
    'mobilenet_v3_small': {'params_m': 2.5, 'base_memory_mb': 200, 'recommended_batch_8gb': 128},
    'mobilenet_v3_large': {'params_m': 5.4, 'base_memory_mb': 350, 'recommended_batch_8gb': 96},
}


class ConfigValidator:
    """é…ç½®æ–‡ä»¶éªŒè¯å™¨"""
    
    # å¿…éœ€å­—æ®µåŠå…¶ç±»å‹
    REQUIRED_FIELDS = {
        'model': str,
        'img_size': int,
        'batch_size': int,
        'epochs': int,
        'lr': float,
    }
    
    # å¯é€‰å­—æ®µåŠå…¶ç±»å‹
    OPTIONAL_FIELDS = {
        'pretrained': bool,
        'data_root': str,
        'num_workers': int,
        'weight_decay': float,
        'loss': str,
        'focal_gamma': float,
        'use_weighted_sampler': bool,
        'sampler': str,  # æ–°å¢ï¼šsampler å­—æ®µ
        'augment_level': str,
        'augmentation': dict,  # æ–°å¢ï¼šåµŒå¥—çš„å¢å¼ºé…ç½®
        'optimizer': str,
        'scheduler': str,
        'warmup_epochs': int,
        'step_size': int,  # æ–°å¢ï¼šStepLR é…ç½®
        'gamma': float,  # æ–°å¢ï¼šscheduler gamma
        'amp': bool,
        'output_dir': str,
        'save_best_only': bool,
        'save_last_interval': int,  # æ–°å¢ï¼šlast checkpoint ä¿å­˜é—´éš”
        'seed': int,
        'val_interval': int,
        'use_albumentations': bool,
        'early_stopping': dict,  # æ–°å¢ï¼šearly stopping é…ç½®
        'focal': dict,  # æ–°å¢ï¼šfocal loss é…ç½®
        'allow_nondeterministic': bool,  # æ–°å¢ï¼šæ€§èƒ½ä¼˜åŒ–é€‰é¡¹
        'allow_tf32': bool,  # æ–°å¢ï¼šTF32 é€‰é¡¹
        'memory_efficient': bool,  # æ–°å¢ï¼šæ˜¾å­˜ä¼˜åŒ–é€‰é¡¹
        'gradient_accumulation_steps': int,  # æ–°å¢ï¼šæ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        'gradient_clipping': bool,  # æ–°å¢ï¼šæ¢¯åº¦è£å‰ª
        'max_grad_norm': float,  # æ–°å¢ï¼šæœ€å¤§æ¢¯åº¦èŒƒæ•°
        'bfloat16': bool,  # æ–°å¢ï¼šbfloat16 æ”¯æŒ
        'use_tensorboard': bool,  # æ–°å¢ï¼šTensorBoard æ”¯æŒ
    }
    
    # æœ‰æ•ˆå€¼çš„æšä¸¾
    VALID_MODELS = [
        'resnet18', 'resnet50', 
        'efficientnet_b0', 'efficientnet_b2',
        'densenet121',
        'mobilenet_v3_small', 'mobilenet_v3_large'
    ]
    
    VALID_LOSSES = ['cross_entropy', 'focal', 'weighted_ce', 'label_smoothing', 'smooth_ce']
    VALID_OPTIMIZERS = ['adam', 'adamw', 'sgd']
    VALID_SCHEDULERS = ['step', 'cosine', 'exponential', 'none']
    VALID_AUGMENT_LEVELS = ['light', 'medium', 'heavy', 'aggressive']
    VALID_SAMPLERS = ['weighted_random', 'random', 'none']
    
    @classmethod
    def validate(cls, config: Dict[str, Any]) -> List[str]:
        """
        éªŒè¯é…ç½®æ–‡ä»¶çš„å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§
        
        Args:
            config: é…ç½®å­—å…¸
        
        Returns:
            è­¦å‘Šä¿¡æ¯åˆ—è¡¨ï¼ˆä¸é˜»æ­¢éªŒè¯é€šè¿‡ï¼‰
        
        Raises:
            ValueError: é…ç½®æ— æ•ˆæ—¶æŠ›å‡ºè¯¦ç»†é”™è¯¯ä¿¡æ¯
        """
        errors: List[str] = []
        warnings: List[str] = []
        
        # 1. æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field, field_type in cls.REQUIRED_FIELDS.items():
            if field not in config:
                errors.append(f"âŒ ç¼ºå°‘å¿…éœ€é…ç½®: {field}")
            elif not isinstance(config.get(field), field_type):
                actual_type = type(config[field]).__name__
                expected_type = field_type.__name__
                errors.append(
                    f"âŒ é…ç½® '{field}' ç±»å‹é”™è¯¯: "
                    f"æœŸæœ› {expected_type}, å®é™… {actual_type}"
                )
        
        # 2. æ£€æŸ¥å¯é€‰å­—æ®µç±»å‹
        for field, field_type in cls.OPTIONAL_FIELDS.items():
            if field in config and not isinstance(config[field], field_type):
                actual_type = type(config[field]).__name__
                expected_type = field_type.__name__
                errors.append(
                    f"âš ï¸ å¯é€‰é…ç½® '{field}' ç±»å‹é”™è¯¯: "
                    f"æœŸæœ› {expected_type}, å®é™… {actual_type}"
                )
        
        # å¦‚æœåŸºæœ¬ç±»å‹æ£€æŸ¥å¤±è´¥ï¼Œæå‰è¿”å›
        if errors:
            raise ValueError("é…ç½®éªŒè¯å¤±è´¥:\n" + "\n".join(errors))
        
        # 3. éªŒè¯æ¨¡å‹åç§°
        model = config['model'].lower()
        if model not in cls.VALID_MODELS:
            errors.append(
                f"âŒ æ— æ•ˆçš„æ¨¡å‹: '{model}'. "
                f"æ”¯æŒçš„æ¨¡å‹: {', '.join(cls.VALID_MODELS)}"
            )
        
        # 4. éªŒè¯è¶…å‚æ•°èŒƒå›´
        img_size = config['img_size']
        if img_size < 32 or img_size > 1024:
            errors.append(f"âŒ img_size ({img_size}) å¿…é¡»åœ¨ [32, 1024] èŒƒå›´å†…")
        
        batch_size = config['batch_size']
        if batch_size < 1 or batch_size > 1024:
            errors.append(f"âŒ batch_size ({batch_size}) å¿…é¡»åœ¨ [1, 1024] èŒƒå›´å†…")
        
        epochs = config['epochs']
        if epochs < 1 or epochs > 1000:
            errors.append(f"âŒ epochs ({epochs}) å¿…é¡»åœ¨ [1, 1000] èŒƒå›´å†…")
        
        lr = config['lr']
        if lr <= 0 or lr > 1:
            errors.append(f"âŒ lr ({lr}) å¿…é¡»åœ¨ (0, 1] èŒƒå›´å†…")
        
        # 5. éªŒè¯å¯é€‰å­—æ®µçš„æœ‰æ•ˆå€¼
        if 'loss' in config:
            loss = config['loss'].lower()
            if loss not in cls.VALID_LOSSES:
                errors.append(
                    f"âŒ æ— æ•ˆçš„loss: '{loss}'. "
                    f"æ”¯æŒ: {', '.join(cls.VALID_LOSSES)}"
                )
        
        if 'optimizer' in config:
            optimizer = config['optimizer'].lower()
            if optimizer not in cls.VALID_OPTIMIZERS:
                errors.append(
                    f"âŒ æ— æ•ˆçš„optimizer: '{optimizer}'. "
                    f"æ”¯æŒ: {', '.join(cls.VALID_OPTIMIZERS)}"
                )
        
        if 'scheduler' in config:
            scheduler = config['scheduler'].lower()
            if scheduler not in cls.VALID_SCHEDULERS:
                errors.append(
                    f"âŒ æ— æ•ˆçš„scheduler: '{scheduler}'. "
                    f"æ”¯æŒ: {', '.join(cls.VALID_SCHEDULERS)}"
                )
        
        if 'augment_level' in config:
            augment_level = config['augment_level'].lower()
            if augment_level not in cls.VALID_AUGMENT_LEVELS:
                errors.append(
                    f"âŒ æ— æ•ˆçš„augment_level: '{augment_level}'. "
                    f"æ”¯æŒ: {', '.join(cls.VALID_AUGMENT_LEVELS)}"
                )
        
        if 'sampler' in config:
            sampler = config['sampler'].lower()
            if sampler not in cls.VALID_SAMPLERS:
                errors.append(
                    f"âŒ æ— æ•ˆçš„sampler: '{sampler}'. "
                    f"æ”¯æŒ: {', '.join(cls.VALID_SAMPLERS)}"
                )
        
        # 6. éªŒè¯åµŒå¥—ç»“æ„
        if 'early_stopping' in config:
            es_cfg = config['early_stopping']
            if not isinstance(es_cfg, dict):
                errors.append("âŒ 'early_stopping' å¿…é¡»æ˜¯å­—å…¸ç±»å‹")
            elif 'patience' not in es_cfg:
                errors.append("âŒ 'early_stopping' ç¼ºå°‘å¿…éœ€å­—æ®µ 'patience'")
            elif not isinstance(es_cfg['patience'], int) or es_cfg['patience'] < 0:
                errors.append("âŒ 'early_stopping.patience' å¿…é¡»æ˜¯éè´Ÿæ•´æ•°")
        
        if 'focal' in config:
            focal_cfg = config['focal']
            if not isinstance(focal_cfg, dict):
                errors.append("âŒ 'focal' å¿…é¡»æ˜¯å­—å…¸ç±»å‹")
            elif 'gamma' in focal_cfg:
                gamma = focal_cfg['gamma']
                if not isinstance(gamma, (int, float)) or gamma < 0:
                    errors.append("âŒ 'focal.gamma' å¿…é¡»æ˜¯éè´Ÿæ•°å€¼")
        
        if 'augmentation' in config:
            aug_cfg = config['augmentation']
            if not isinstance(aug_cfg, dict):
                errors.append("âŒ 'augmentation' å¿…é¡»æ˜¯å­—å…¸ç±»å‹")
        
        # 7. éªŒè¯é€»è¾‘ä¸€è‡´æ€§ï¼ˆè­¦å‘Šï¼Œä¸é˜»æ­¢éªŒè¯é€šè¿‡ï¼‰
        if 'focal_gamma' in config and config.get('loss', '').lower() != 'focal':
            warnings.append(
                "âš ï¸ é…ç½®äº†focal_gammaä½†lossä¸æ˜¯'focal'ï¼Œfocal_gammaå°†è¢«å¿½ç•¥"
            )
        
        if 'focal' in config and config.get('loss', '').lower() != 'focal':
            warnings.append(
                "âš ï¸ é…ç½®äº†focalä½†lossä¸æ˜¯'focal'ï¼Œfocalé…ç½®å°†è¢«å¿½ç•¥"
            )
        
        if 'weight_decay' in config:
            weight_decay = config['weight_decay']
            if weight_decay < 0 or weight_decay > 1:
                errors.append(
                    f"âŒ weight_decay ({weight_decay}) å¿…é¡»åœ¨ [0, 1] èŒƒå›´å†…"
                )
        
        if 'num_workers' in config:
            num_workers = config['num_workers']
            if num_workers < 0 or num_workers > 32:
                warnings.append(
                    f"âš ï¸ num_workers ({num_workers}) è¶…å‡ºæ¨èèŒƒå›´ [0, 32]ï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½é—®é¢˜"
                )
        
        # å¦‚æœæœ‰é”™è¯¯ï¼ŒæŠ›å‡ºå¼‚å¸¸
        if errors:
            raise ValueError("é…ç½®éªŒè¯å¤±è´¥:\n" + "\n".join(errors))
        
        # æ‰“å°è­¦å‘Šï¼ˆä¸é˜»æ­¢éªŒè¯é€šè¿‡ï¼‰
        if warnings:
            for warning in warnings:
                print(warning)
        
        print("[OK] é…ç½®éªŒè¯é€šè¿‡")
        return warnings
    
    @classmethod
    def estimate_gpu_memory(cls, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¼°ç®— GPU æ˜¾å­˜éœ€æ±‚ã€‚
        
        Args:
            config: é…ç½®å­—å…¸
        
        Returns:
            åŒ…å«æ˜¾å­˜ä¼°ç®—ä¿¡æ¯çš„å­—å…¸
        """
        model = config.get('model', 'resnet18').lower()
        img_size = config.get('img_size', 224)
        batch_size = config.get('batch_size', 16)
        use_amp = config.get('amp', False)
        
        # è·å–æ¨¡å‹è§„æ ¼
        specs = MODEL_SPECS.get(model, MODEL_SPECS['resnet18'])
        
        # ä¼°ç®—æ˜¾å­˜ï¼ˆMBï¼‰
        # å…¬å¼ï¼šbase_memory + (batch_size * img_size^2 * 3 * 4 / 1e6) * factor
        # factor è€ƒè™‘æ¿€æ´»å€¼ã€æ¢¯åº¦ç­‰
        pixel_memory = batch_size * (img_size ** 2) * 3 * 4 / 1e6  # è¾“å…¥å¼ é‡
        activation_factor = 4.0  # æ¿€æ´»å€¼å’Œæ¢¯åº¦å¤§çº¦æ˜¯è¾“å…¥çš„ 4 å€
        
        estimated_memory = specs['base_memory_mb'] + pixel_memory * activation_factor
        
        # AMP å¯ä»¥å‡å°‘çº¦ 40% æ˜¾å­˜
        if use_amp:
            estimated_memory *= 0.6
        
        # æ¨èé…ç½®
        recommended_batch = specs['recommended_batch_8gb']
        if img_size > 256:
            recommended_batch = int(recommended_batch * (256 / img_size) ** 2)
        
        return {
            'model': model,
            'estimated_memory_mb': round(estimated_memory),
            'estimated_memory_gb': round(estimated_memory / 1024, 2),
            'recommended_batch_8gb': recommended_batch,
            'current_batch_size': batch_size,
            'amp_enabled': use_amp,
            'warning': estimated_memory > 7000,  # æ¥è¿‘ 8GB æ—¶è­¦å‘Š
            'suggestions': cls._get_memory_suggestions(estimated_memory, batch_size, use_amp)
        }
    
    @classmethod
    def _get_memory_suggestions(cls, memory_mb: float, batch_size: int, use_amp: bool) -> List[str]:
        """ç”Ÿæˆæ˜¾å­˜ä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        if memory_mb > 10000:
            suggestions.append(f"âš ï¸ é¢„è®¡æ˜¾å­˜è¶…è¿‡ 10GBï¼Œå»ºè®®å‡å° batch_sizeï¼ˆå½“å‰: {batch_size}ï¼‰")
        
        if memory_mb > 7000 and not use_amp:
            suggestions.append("ğŸ’¡ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (amp: true) å¯å‡å°‘çº¦ 40% æ˜¾å­˜")
        
        if batch_size > 32 and memory_mb > 6000:
            suggestions.append(f"ğŸ’¡ è€ƒè™‘å‡å° batch_size åˆ° 16-32 ä»¥æé«˜ç¨³å®šæ€§")
        
        return suggestions
    
    @classmethod
    def check_cli_conflicts(cls, config: Dict[str, Any], cli_args: Dict[str, Any]) -> List[str]:
        """
        æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°ä¸é…ç½®æ–‡ä»¶çš„å†²çªã€‚
        
        Args:
            config: é…ç½®æ–‡ä»¶å­—å…¸
            cli_args: å‘½ä»¤è¡Œå‚æ•°å­—å…¸
        
        Returns:
            å†²çªè­¦å‘Šåˆ—è¡¨
        """
        conflicts = []
        
        # æ£€æŸ¥è¦†ç›–çš„å‚æ•°
        override_keys = ['epochs', 'batch_size', 'lr', 'model', 'augment_level']
        
        for key in override_keys:
            if cli_args.get(key) is not None and key in config:
                cli_val = cli_args[key]
                cfg_val = config[key]
                if cli_val != cfg_val:
                    conflicts.append(
                        f"â„¹ï¸ '{key}' å°†è¢«å‘½ä»¤è¡Œå‚æ•°è¦†ç›–: {cfg_val} â†’ {cli_val}"
                    )
        
        return conflicts
    
    @classmethod
    def merge_configs(cls, base_config: Dict[str, Any], override_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆå¹¶ä¸¤ä¸ªé…ç½®å­—å…¸ï¼ˆæ”¯æŒé…ç½®ç»§æ‰¿ï¼‰ã€‚
        
        Args:
            base_config: åŸºç¡€é…ç½®
            override_config: è¦†ç›–é…ç½®
        
        Returns:
            åˆå¹¶åçš„é…ç½®
        """
        result = base_config.copy()
        
        for key, value in override_config.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                # é€’å½’åˆå¹¶å­—å…¸
                result[key] = cls.merge_configs(result[key], value)
            else:
                result[key] = value
        
        return result
    
    @classmethod
    def validate_file(cls, config_path: str) -> Dict[str, Any]:
        """
        ä»æ–‡ä»¶åŠ è½½å¹¶éªŒè¯é…ç½®
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
        Returns:
            éªŒè¯åçš„é…ç½®å­—å…¸
        
        Raises:
            FileNotFoundError: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨
            ValueError: é…ç½®æ— æ•ˆ
        """
        config_file = Path(config_path)
        
        if not config_file.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
        except yaml.YAMLError as e:
            raise ValueError(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        
        if config is None:
            raise ValueError("é…ç½®æ–‡ä»¶ä¸ºç©º")
        
        cls.validate(config)
        
        return config


def load_and_validate_config(config_path: str) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šåŠ è½½å¹¶éªŒè¯é…ç½®æ–‡ä»¶
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
    
    Returns:
        éªŒè¯åçš„é…ç½®å­—å…¸
    
    Example:
        >>> config = load_and_validate_config('configs/model.yaml')
        âœ… é…ç½®éªŒè¯é€šè¿‡
    """
    return ConfigValidator.validate_file(config_path)


if __name__ == '__main__':
    # æµ‹è¯•ç¤ºä¾‹
    import sys
    
    if len(sys.argv) > 1:
        config_path = sys.argv[1]
        try:
            config = load_and_validate_config(config_path)
            print(f"\né…ç½®æ–‡ä»¶æœ‰æ•ˆ: {config_path}")
            print("\nä¸»è¦é…ç½®:")
            print(f"  Model: {config['model']}")
            print(f"  Image Size: {config['img_size']}")
            print(f"  Batch Size: {config['batch_size']}")
            print(f"  Epochs: {config['epochs']}")
            print(f"  Learning Rate: {config['lr']}")
        except (FileNotFoundError, ValueError) as e:
            print(f"\né”™è¯¯: {e}", file=sys.stderr)
            sys.exit(1)
    else:
        print("ç”¨æ³•: python src/utils/config_validator.py <config_file>")
        print("ç¤ºä¾‹: python src/utils/config_validator.py src/configs/final_model.yaml")

